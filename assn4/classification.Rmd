---
title: "Assignment 4: cross-validation, KNN, SVM, NC"
subtitle: "Does your iPhone know what you're doing?"
author: "James Barbour"
output: html_document
---


```{r, message=FALSE, warning=F}
library(tidyverse)

library(class) # KNN
library(e1071) # SVM
library(kernlab) # kernel SVM
library(caret) # tuning
library(stringr)

train <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/human_activity_train.csv')

test <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/human_activity_test.csv')

# only consider walking upstairs vs downstairs
train <- train %>% 
  filter(activity == 2 | activity == 3)

test <- test %>% 
  filter(activity == 2 | activity == 3)

# train$activity[train$activity == 2] <- -1
# train$activity[train$activity == 3] <- 1
# test$activity[test$activity == 2] <- -1
# test$activity[test$activity == 3] <- 1

# subsample the data
set.seed(8599)
train <- train[sample(x=1:dim(train)[1], size=200), ]
test <- test[sample(x=1:dim(test)[1], size=200), ]
```

```{r, echo=F}
knn_tuning_error_plot <- function(train, test, k_cv, k_values, cv_seed=NA) {
  # Returns the tuning error plots for KNN with the three tuning error curves
  # train, CV, and test error
  # train and test: are the train and test data
  # both are a data frame with the same column names
  # one column in named y which is the class labels
  # k_cv: is the number of cross validation folds
  # k_values: is the sequence of K values try for KNN
  # cv_seed: is the seed for the cross validation folds
  # returns a ggplot object    
  
  # set seed if it is given
  if(!is.na(cv_seed)){
    set.seed(cv_seed)
  }
  
  train_x <- train %>% select(-activity)
  train_y <- train$activity
  test_x <- test %>% select(-activity)
  test_y <- test$activity
  
  test_error <- lapply(k_values, function(k) mean(knn(train_x, test_x, train_y, k) != test_y)) %>%
    unlist()
  
  train_error <- lapply(k_values, function(k) mean(knn(train_x, train_x, train_y, k) != train_y)) %>%
    unlist()
  
  # helpful quantities
  num_k <- length(k_values)
  n <- dim(train)[1]
  
  # create data frame to store CV errors
  cv_error_df <- matrix(0, nrow=num_k, ncol=k_cv) %>% 
    as_tibble() %>% 
    add_column(k=k_values)
  colnames(cv_error_df) <- str_replace(colnames(cv_error_df), 'V', 'fold')
  
  # for each of the M folds
  for(m in 1:k_cv){
    
    # number of points that go in the cv train set
    n_cv_tr <- floor(n * (k_cv-1)/k_cv)
    
    # randomly select n_tr numbers, without replacement, from 1...n
    cv_tr_indices <- sample(x=1:n, size=n_cv_tr, replace=FALSE)
    
    # break the data into a non-overlapping train and test set
    cv_tr_data <- train[cv_tr_indices, ]
    cv_tst_data <- train[-cv_tr_indices, ]
    
    
    # break the train/test data into x matrix and y vectors
    # this formatting is useful for the knn() functions
    cv_tr_x <- cv_tr_data %>% select(-activity)
    cv_tr_y <- cv_tr_data$activity
    
    cv_tst_x <- cv_tst_data %>% select(-activity)
    cv_tst_y <- cv_tst_data$activity # turn into a vector
    
    # for each value of k
    for(i in 1:num_k){
      
      # fix k for this loop iteration
      k <- k_values[i]
      
      # get predictions on cv test data data
      cv_tst_predictions <- knn(train=cv_tr_x, # training x
                                test=cv_tst_x, # test x
                                cl=cv_tr_y, # train y
                                k=k) # set k
      
      # compute error rate on cv-test data
      cv_tst_err <- mean(cv_tst_y != cv_tst_predictions)
      
      # store values in the data frame
      cv_error_df[i, paste0('fold',m)] <- cv_tst_err
    }
  }
  
  
  error_df <- tibble(k=k_values,
                     tr=train_error,
                     tst=test_error,
                     cv=rowMeans(select(cv_error_df, -k))) 
  
  return(error_df %>% 
           gather(key='type', value='error', tr, tst, cv) %>% 
           ggplot() +
           geom_point(aes(x=k, y=error, color=type, shape=type)) +
           geom_line(aes(x=k, y=error, color=type, linetype=type)))
}

```

```{r, echo=F}
nearest_centroid <- function(train, test_x){
  # returns the predictions for nearest centroid on a test set
  # train_x and test_x: are the train/test x data
  # assume these are both numerical matrices with the same number of columns
  # train_y: is a vector of class labels for the training data
  # return a vector of predicted class labels for the test data
  # fit_mean_difference() ???
  obs_means <- train %>%
    group_by(activity) %>%
    summarize_all(mean)
  
  mean_pos <- select(filter(obs_means, y == 2), -y)
  mean_neg <- select(filter(obs_means, y == 3), -y)
  
  test_x <- test_x %>%
    add_column(
      dist_pos = apply(test_x, 1, function(x) sqrt(sum((x - mean_pos)^2))),
      dist_neg = apply(test_x, 1, function(x) sqrt(sum((x - mean_neg)^2)))
    )
  
  test_x %>% 
    mutate(y_pred = ifelse(dist_pos < dist_neg, 3, 2)) %>% 
    mutate(y_pred = factor(y_pred)) 
}
```

```{r}
# use these k values
k_values <- seq(from=1, to=41, by=2)

# Q1: KNN Test set error --------------------------------------------------
train_x <- train %>% select(-activity)
train_y <- train$activity
test_x <- test %>% select(-activity)
test_y <- test$activity

# 1a:
test_error <- lapply(k_values, function(k) mean(knn(train_x, test_x, train_y, k) != test_y)) %>%
  unlist()

# number of cross-validation folds
M <- 10

# helpful quantities
num_k <- length(k_values)
n <- dim(train)[1]

# create data frame to store CV errors
cv_error_df <- matrix(0, nrow=num_k, ncol=M) %>% 
  as_tibble() %>% 
  add_column(k=k_values)
colnames(cv_error_df) <- str_replace(colnames(cv_error_df), 'V', 'fold')

# for each of the M folds
for(m in 1:M){
  
  # number of points that go in the cv train set
  n_cv_tr <- floor(n * (M-1)/M)
  
  # randomly select n_tr numbers, without replacement, from 1...n
  cv_tr_indices <- sample(x=1:n, size=n_cv_tr, replace=FALSE)
  
  # break the data into a non-overlapping train and test set
  cv_tr_data <- train[cv_tr_indices, ]
  cv_tst_data <- train[-cv_tr_indices, ]
  
  
  # break the train/test data into x matrix and y vectors
  # this formatting is useful for the knn() functions
  cv_tr_x <- cv_tr_data %>% select(-activity)
  cv_tr_y <- cv_tr_data$activity
  
  cv_tst_x <- cv_tst_data %>% select(-activity)
  cv_tst_y <- cv_tst_data$activity # turn into a vector
  
  # for each value of k
  for(i in 1:num_k){
    
    # fix k for this loop iteration
    k <- k_values[i]
    
    # get predictions on cv test data data
    cv_tst_predictions <- knn(train=cv_tr_x, # training x
                              test=cv_tst_x, # test x
                              cl=cv_tr_y, # train y
                              k=k) # set k
    
    # compute error rate on cv-test data
    cv_tst_err <- mean(cv_tst_y != cv_tst_predictions)
    
    # store values in the data frame
    cv_error_df[i, paste0('fold',m)] <- cv_tst_err
  }
}

error_df <- tibble(k=k_values,
                   tst=test_error,
                   cv=rowMeans(select(cv_error_df, -k))) 

# 1b:
error_df %>% 
  gather(key='type', value='error', tst, cv) %>% 
  ggplot() +
  geom_point(aes(x=k, y=error, color=type, shape=type)) +
  geom_line(aes(x=k, y=error, color=type, linetype=type))

# 1c:
# The best value of K is 2

# 1d:

# Q2: What happens when we change the number of folds ---------------------
# 2a:
train_error <- lapply(k_values, function(k) mean(knn(train_x, train_x, train_y, k) != train_y)) %>%
  unlist()

# 2b:
error_df <- error_df %>%
  mutate(tr=train_error)
error_df %>% 
  gather(key='type', value='error', tr, tst, cv) %>% 
  ggplot() +
  geom_point(aes(x=k, y=error, color=type, shape=type)) +
  geom_line(aes(x=k, y=error, color=type, linetype=type))

# 2c: see above

# 2d:
lapply(c(5, 10, 20, 50), function(x) knn_tuning_error_plot(train, test, x, k_values, 8599))


# Q3: Nearest Centroid ----------------------------------------------------
# 3a: see above
# 3b:
res <- nearest_centroid(train, test_x)
res
```

```{r}
# Q4: Linear SVM ----------------------------------------------------------
C_values <- tibble(C=10^seq(from=-5, to=5, by=1))

trControl_5_fold <- trainControl(method = "cv",
                          number = 5)

trControl_10_fold <- trainControl(method = "cv",
                          number = 10)

tune_grid <- expand.grid(C=C_values,
                         degree=c(1),
                         scale=1)

# fit the SVM model
tuned_svm_5_fold <- train(x=train_x,
                   y=train_y,
                   tuneGrid = tune_grid,
                   trControl = trControl,
                   method = "svmLinear")

tuned_svm_5_fold <- train(x=train_x,
                   y=train_y,
                   method = "svmPoly",
                   tuneGrid = tune_grid,
                   trControl = trControl,
                   method = "svmLinear")

tuned_svm

trControl <- trainControl(method = "cv", # perform cross validation
                          number = 5)


# Q5: Radial Kernel SVM ---------------------------------------------------
```