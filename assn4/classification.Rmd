---
title: "Assignment 4: cross-validation, KNN, SVM, NC"
subtitle: "Does your iPhone know what you're doing?"
author: "James Barbour"
output: html_document
---


```{r, message=FALSE, warning=F}
require(tidyverse)
require(class)
require(e1071)
require(kernlab)
require(caret)
require(stringr)

train <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/human_activity_train.csv')

test <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/human_activity_test.csv')

# only consider walking upstairs vs downstairs
train <- train %>% 
  rename(y = activity) %>%
  filter(y == 2 | y == 3)

test <- test %>% 
  rename(y = activity) %>%
  filter(y == 2 | y == 3)

# train$y[train$y == 2] <- -1
# train$y[train$y == 3] <- 1
# test$y[test$y == 2] <- -1
# test$y[test$y == 3] <- 1

# subsample the data
set.seed(8599)
train <- train[sample(x=1:dim(train)[1], size=200), ]
test <- test[sample(x=1:dim(test)[1], size=200), ]
```

```{r, echo=F}
knn_tuning_error_plot <- function(train, test, k_cv, k_values, cv_seed=NA) {
  # Returns the tuning error plots for KNN with the three tuning error curves
  # train, CV, and test error
  # train and test: are the train and test data
  # both are a data frame with the same column names
  # one column in named y which is the class labels
  # k_cv: is the number of cross validation folds
  # k_values: is the sequence of K values try for KNN
  # cv_seed: is the seed for the cross validation folds
  # returns a ggplot object    
  
  # set seed if it is given
  if(!is.na(cv_seed)){
    set.seed(cv_seed)
  }
  
  train_x <- train %>% select(-y)
  train_y <- train$y
  test_x <- test %>% select(-y)
  test_y <- test$y
  
  test_error <- lapply(k_values, function(k) mean(knn(train_x, test_x, train_y, k) != test_y)) %>%
    unlist()
  
  train_error <- lapply(k_values, function(k) mean(knn(train_x, train_x, train_y, k) != train_y)) %>%
    unlist()
  
  # helpful quantities
  num_k <- length(k_values)
  n <- dim(train)[1]
  
  # create data frame to store CV errors
  cv_error_df <- matrix(0, nrow=num_k, ncol=k_cv) %>% 
    as_tibble() %>% 
    add_column(k=k_values)
  colnames(cv_error_df) <- str_replace(colnames(cv_error_df), 'V', 'fold')
  
  # for each of the M folds
  for(m in 1:k_cv){
    
    # number of points that go in the cv train set
    n_cv_tr <- floor(n * (k_cv-1)/k_cv)
    
    # randomly select n_tr numbers, without replacement, from 1...n
    cv_tr_indices <- sample(x=1:n, size=n_cv_tr, replace=FALSE)
    
    # break the data into a non-overlapping train and test set
    cv_tr_data <- train[cv_tr_indices, ]
    cv_tst_data <- train[-cv_tr_indices, ]
    
    
    # break the train/test data into x matrix and y vectors
    # this formatting is useful for the knn() functions
    cv_tr_x <- cv_tr_data %>% select(-y)
    cv_tr_y <- cv_tr_data$y
    
    cv_tst_x <- cv_tst_data %>% select(-y)
    cv_tst_y <- cv_tst_data$y # turn into a vector
    
    # for each value of k
    for(i in 1:num_k){
      
      # fix k for this loop iteration
      k <- k_values[i]
      
      # get predictions on cv test data data
      cv_tst_predictions <- knn(train=cv_tr_x, # training x
                                test=cv_tst_x, # test x
                                cl=cv_tr_y, # train y
                                k=k) # set k
      
      # compute error rate on cv-test data
      cv_tst_err <- mean(cv_tst_y != cv_tst_predictions)
      
      # store values in the data frame
      cv_error_df[i, paste0('fold',m)] <- cv_tst_err
    }
  }
  
  
  error_df <- tibble(k=k_values,
                     tr=train_error,
                     tst=test_error,
                     cv=rowMeans(select(cv_error_df, -k))) 
  
  return(error_df %>% 
           gather(key='type', value='error', tr, tst, cv) %>% 
           ggplot() +
           geom_point(aes(x=k, y=error, color=type, shape=type)) +
           geom_line(aes(x=k, y=error, color=type, linetype=type)))
}

```

```{r, echo=F}
nearest_centroid <- function(train, test){
  # returns the predictions for nearest centroid on a test set
  # train_x and test_x: are the train/test x data
  # assume these are both numerical matrices with the same number of columns
  # train_y: is a vector of class labels for the training data
  # return a vector of predicted class labels for the test data
  # fit_mean_difference() ???
  obs_means <- train %>%
    group_by(y) %>%
    summarize_all(mean)
  
  mean_pos <- select(filter(obs_means, y == 2), -y)
  mean_neg <- select(filter(obs_means, y == 3), -y)
  
  test <- test %>%
    add_column(
      dist_pos = apply(test, 1, function(x) sqrt(sum((x - mean_pos)^2))),
      dist_neg = apply(test, 1, function(x) sqrt(sum((x - mean_neg)^2)))
    )
  
  test %>% 
    mutate(y_pred = ifelse(dist_pos < dist_neg, 3, 2)) %>% 
    as_tibble()
}
```

```{r}
# use these k values
k_values <- seq(from=1, to=41, by=2)

# Q1: KNN Test set error --------------------------------------------------
train_x <- train %>% select(-y)
train_y <- train$y
test_x <- test %>% select(-y)
test_y <- test$y

# 1a:
test_error <- lapply(k_values, function(k) mean(knn(train_x, test_x, train_y, k) != test_y)) %>%
  unlist()
test_error

# number of cross-validation folds
M <- 10

# helpful quantities
num_k <- length(k_values)
n <- dim(train)[1]

# create data frame to store CV errors
cv_error_df <- matrix(0, nrow=num_k, ncol=M) %>% 
  as_tibble() %>% 
  add_column(k=k_values)
colnames(cv_error_df) <- str_replace(colnames(cv_error_df), 'V', 'fold')

# for each of the M folds
for(m in 1:M){
  
  # number of points that go in the cv train set
  n_cv_tr <- floor(n * (M-1)/M)
  
  # randomly select n_tr numbers, without replacement, from 1...n
  cv_tr_indices <- sample(x=1:n, size=n_cv_tr, replace=FALSE)
  
  # break the data into a non-overlapping train and test set
  cv_tr_data <- train[cv_tr_indices, ]
  cv_tst_data <- train[-cv_tr_indices, ]
  
  
  # break the train/test data into x matrix and y vectors
  # this formatting is useful for the knn() functions
  cv_tr_x <- cv_tr_data %>% select(-y)
  cv_tr_y <- cv_tr_data$y
  
  cv_tst_x <- cv_tst_data %>% select(-y)
  cv_tst_y <- cv_tst_data$y # turn into a vector
  
  # for each value of k
  for(i in 1:num_k){
    
    # fix k for this loop iteration
    k <- k_values[i]
    
    # get predictions on cv test data data
    cv_tst_predictions <- knn(train=cv_tr_x, # training x
                              test=cv_tst_x, # test x
                              cl=cv_tr_y, # train y
                              k=k) # set k
    
    # compute error rate on cv-test data
    cv_tst_err <- mean(cv_tst_y != cv_tst_predictions)
    
    # store values in the data frame
    cv_error_df[i, paste0('fold',m)] <- cv_tst_err
  }
}

error_df <- tibble(k=k_values,
                   tst=test_error,
                   cv=rowMeans(select(cv_error_df, -k))) 

# 1b:
error_df %>% 
  gather(key='type', value='error', tst, cv) %>% 
  ggplot() +
  geom_point(aes(x=k, y=error, color=type, shape=type)) +
  geom_line(aes(x=k, y=error, color=type, linetype=type))

# 1c:
# The best value of K is 2

# 1d:
# test validation remains fairly consistent between 0.13 and 0.21, 
# but cross-validation increases from 0.1 to 0.25. Does come fairly close
# to test validation between k=10 and k=23

# Q2: What happens when we change the number of folds ---------------------
# 2a:
train_error <- lapply(k_values, function(k) mean(knn(train_x, train_x, train_y, k) != train_y)) %>%
  unlist()
train_error

# 2b:
error_df <- error_df %>%
  mutate(tr=train_error)
error_df %>% 
  gather(key='type', value='error', tr, tst, cv) %>% 
  ggplot() +
  geom_point(aes(x=k, y=error, color=type, shape=type)) +
  geom_line(aes(x=k, y=error, color=type, linetype=type))

# 2c: see above

# 2d:
lapply(c(5, 10, 20, 50), function(x) knn_tuning_error_plot(train, test, x, k_values, 8599))

# Q3: Nearest Centroid ----------------------------------------------------
# 3a: see above
# 3b:
# train error
centroid_train <- nearest_centroid(train, train)
centroid_train %>% 
  summarise(error = mean(y != y_pred))

# test error
centroid_test <- nearest_centroid(train, test)
centroid_test %>% 
  summarise(error = mean(y != y_pred))

# Q4: Linear SVM ----------------------------------------------------------
train_y <- factor(train_y)
C_values <- tibble(C=10^seq(from=-5, to=5, by=1))

prev_w = oldw <- getOption("warn")
options(warn = -1)

# fit the SVM model
tuned_svm_5_fold <- train(x = train_x,
                          y = train_y,
                          method = "svmLinear",
                          tuneGrid = C_values,
                          trControl = trainControl(method = "cv", number = 5),
                          metric = "Accuracy")

tuned_svm_10_fold <- train(x = train_x,
                           y = train_y,
                           method = "svmLinear",
                           tuneGrid = C_values,
                           trControl = trainControl(method = "cv", number = 10),
                           metric = "Accuracy")

options(warn = oldw)

tuned_svm_5_fold$bestTune
tuned_svm_10_fold$bestTune
tuned_svm_5_fold
tuned_svm_10_fold

# Best model: C = 1e-2, 10 folds

svmfit <- svm(y ~ .,
              data=train,
              cost=.01,
              scale=FALSE,
              type='C-classification',
              shrinking=FALSE,
              kernel='linear') 
svmfit

# train error
svm_train_predictions <- predict(svmfit, newdata = train)
train %>% 
  mutate(svm_y_pred = svm_train_predictions) %>%
  summarise(error = mean(y != svm_y_pred))

# test error
svm_test_predictions <- predict(svmfit, newdata = test)
test %>% 
  mutate(svm_y_pred = svm_test_predictions) %>% 
  summarise(error = mean(y != svm_y_pred))

# Q5: Radial Kernel SVM ---------------------------------------------------
tune_grid <- expand.grid(C = 10^seq(from=-5, to=5, by=1),
                         sigma = 10^seq(from=-5, to=5, by=1))

oldw <- getOption('warn')
options(warn = -1)

# fit the SVM model
tuned_svm_rad <- train(x = train_x,
                       y = train_y,
                       method = "svmRadial",
                       tuneGrid = tune_grid,
                       trControl = trainControl(method = "cv", number = 5),
                       metric = "Accuracy")


options(warn = oldw)

tuned_svm_rad$bestTune

# Best model: sigma = 0.001, C = 10, 10 folds
svmfit_rad <- svm(y ~ .,
                  data=train,
                  cost=10,
                  sigma=0.001,
                  scale=FALSE,
                  type='C-classification',
                  shrinking=FALSE,
                  kernel='radial') 
svmfit_rad

# train error
svm_rad_train_predictions <- predict(svmfit_rad, newdata = train)
train %>% 
  mutate(svm_rad_y_pred = svm_rad_train_predictions) %>%
  summarise(error = mean(y != svm_rad_y_pred))

# test error
svm_rad_test_predictions <- predict(svmfit_rad, newdata = test)
test %>% 
  mutate(svm_rad_y_pred = svm_rad_test_predictions) %>% 
  summarise(error = mean(y != svm_rad_y_pred))
```













