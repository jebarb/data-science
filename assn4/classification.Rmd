---
title: "Assignment 4: cross-validation, KNN, SVM, NC"
subtitle: "Does your iPhone know what you're doing?"
author: "James Barbour"
output: html_document
---


```{r, message=FALSE, warning=F}
library(tidyverse)

library(class) # KNN
library(e1071) # SVM
library(kernlab) # kernel SVM
library(caret) # tuning
library(stringr)

train <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/human_activity_train.csv')

test <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/human_activity_test.csv')

# only consider walking upstairs vs downstairs
train <- train %>% 
  filter(activity == 2 | activity == 3)

test <- test %>% 
  filter(activity == 2 | activity == 3)

# subsample the data
set.seed(8599)
train <- train[sample(x=1:dim(train)[1], size=200), ]
test <- test[sample(x=1:dim(test)[1], size=200), ]
```

```{r, echo=F}
knn_tuning_error_plot <- function(train, test, k_cv, k_values, cv_seed=NA) {
  # Returns the tuning error plots for KNN with the three tuning error curves
  # train, CV, and test error
  # train and test: are the train and test data
  # both are a data frame with the same column names
  # one column in named y which is the class labels
  # k_cv: is the number of cross validation folds
  # k_values: is the sequence of K values try for KNN
  # cv_seed: is the seed for the cross validation folds
  # returns a ggplot object    
  
  # set seed if it is given
  if(!is.na(cv_seed)){
    set.seed(cv_seed)
  }
  
  train_x <- train %>% select(-activity)
  train_y <- train$activity
  test_x <- test %>% select(-activity)
  test_y <- test$activity
  
  test_error <- lapply(k_values, function(k) mean(knn(train_x, test_x, train_y, k) != test_y)) %>%
    unlist()
  
  train_error <- lapply(k_values, function(k) mean(knn(train_x, train_x, train_y, k) != train_y)) %>%
    unlist()
  
  # helpful quantities
  num_k <- length(k_values)
  n <- dim(train)[1]
  
  # create data frame to store CV errors
  cv_error_df <- matrix(0, nrow=num_k, ncol=M) %>% 
    as_tibble() %>% 
    add_column(k=k_values)
  colnames(cv_error_df) <- str_replace(colnames(cv_error_df), 'V', 'fold')
  
  # for each of the M folds
  for(cv in 1:k_cv){
    
    # number of points that go in the cv train set
    n_cv_tr <- floor(n * (k_cv-1)/k_cv)
    
    # randomly select n_tr numbers, without replacement, from 1...n
    cv_tr_indices <- sample(x=1:n, size=n_cv_tr, replace=FALSE)
    
    # break the data into a non-overlapping train and test set
    cv_tr_data <- train[cv_tr_indices, ]
    cv_tst_data <- train[-cv_tr_indices, ]
    
    # break the train/test data into x matrix and y vectors
    # this formatting is useful for the knn() functions
    cv_tr_x <- cv_tr_data %>% select(-activity)
    cv_tr_y <- cv_tr_data$activity
    
    cv_tst_x <- cv_tst_data %>% select(-activity)
    cv_tst_y <- cv_tst_data$activity # turn into a vector
    
    # for each value of k
    for(i in 1:num_k){
      
      # fix k for this loop iteration
      k <- k_values[i]
      
      # get predictions on cv test data data
      cv_tst_predictions <- knn(train=cv_tr_x, # training x
                                test=cv_tst_x, # test x
                                cl=cv_tr_y, # train y
                                k=k) # set k
      
      # compute error rate on cv-test data
      cv_tst_err <- mean(cv_tst_y != cv_tst_predictions)
      
      # store values in the data frame
      cv_error_df[i, paste0('fold',m)] <- cv_tst_err
    }
  }
  
  error_df <- tibble(k=k_values,
                     tr=train_error,
                     tst=test_error,
                     cv=rowMeans(select(cv_error_df, -k))) 
  
  return(error_df %>% 
           gather(key='type', value='error', tr, tst, cv) %>% 
           ggplot() +
           geom_point(aes(x=k, y=error, color=type, shape=type)) +
           geom_line(aes(x=k, y=error, color=type, linetype=type)))
}

```


```{r}
# use these k values
k_values <- seq(from=1, to=41, by=2)

# Q1: KNN Test set error --------------------------------------------------
train_x <- train %>% select(-activity)
train_y <- train$activity
test_x <- test %>% select(-activity)
test_y <- test$activity

# 1a:
test_error <- lapply(k_values, function(k) mean(knn(train_x, test_x, train_y, k) != test_y)) %>%
  unlist()

# number of cross-validation folds
M <- 10

# helpful quantities
num_k <- length(k_values)
n <- dim(train)[1]

# create data frame to store CV errors
cv_error_df <- matrix(0, nrow=num_k, ncol=M) %>% 
  as_tibble() %>% 
  add_column(k=k_values)
colnames(cv_error_df) <- str_replace(colnames(cv_error_df), 'V', 'fold')

# seed for CV subsampling
set.seed(345)

# for each of the M folds
for(m in 1:M){
  
  # number of points that go in the cv train set
  n_cv_tr <- floor(n * (M-1)/M)
  
  # randomly select n_tr numbers, without replacement, from 1...n
  cv_tr_indices <- sample(x=1:n, size=n_cv_tr, replace=FALSE)
  
  # break the data into a non-overlapping train and test set
  cv_tr_data <- train[cv_tr_indices, ]
  cv_tst_data <- train[-cv_tr_indices, ]
  
  
  # break the train/test data into x matrix and y vectors
  # this formatting is useful for the knn() functions
  cv_tr_x <- cv_tr_data %>% select(-activity)
  cv_tr_y <- cv_tr_data$activity
  
  cv_tst_x <- cv_tst_data %>% select(-activity)
  cv_tst_y <- cv_tst_data$activity # turn into a vector
  
  # for each value of k
  for(i in 1:num_k){
    
    # fix k for this loop iteration
    k <- k_values[i]
    
    # get predictions on cv test data data
    cv_tst_predictions <- knn(train=cv_tr_x, # training x
                              test=cv_tst_x, # test x
                              cl=cv_tr_y, # train y
                              k=k) # set k
    
    # compute error rate on cv-test data
    cv_tst_err <- mean(cv_tst_y != cv_tst_predictions)
    
    # store values in the data frame
    cv_error_df[i, paste0('fold',m)] <- cv_tst_err
  }
}

error_df <- tibble(k=k_values,
                   tst=test_error,
                   cv=rowMeans(select(cv_error_df, -k))) 

# 1b:
error_df %>% 
  gather(key='type', value='error', tst, cv) %>% 
  ggplot() +
  geom_point(aes(x=k, y=error, color=type, shape=type)) +
  geom_line(aes(x=k, y=error, color=type, linetype=type))

# 1c:
# The best value of K is 1

# 1d:

# Q2: What happens when we change the number of folds ---------------------
# 2a:
train_error <- lapply(k_values, function(k) mean(knn(train_x, train_x, train_y, k) != train_y)) %>%
  unlist()

# 2b:
error_df <- error_df %>%
  mutate(tr=train_error)
error_df %>% 
  gather(key='type', value='error', tr, tst, cv) %>% 
  ggplot() +
  geom_point(aes(x=k, y=error, color=type, shape=type)) +
  geom_line(aes(x=k, y=error, color=type, linetype=type))

# 2c: see above

# 2d:
lapply(c(5, 10, 20, 50), function(x) knn_tuning_error_plot(train, test, x, k_values, 345)) %>%
  unlist()


# Q3: Nearest Centroid ----------------------------------------------------

# Q4: Linear SVM ----------------------------------------------------------

# Q5: Radial Kernel SVM ---------------------------------------------------
```